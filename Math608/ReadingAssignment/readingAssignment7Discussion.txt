The paper explores the ethical issues surrounding using machine learning algorithms in risk assessment, particularly in the criminal justice system. The paper highlights two case studies, one with the biased YouTube algorithm and the second with the Northpointe-made software COMPAS.
In the YouTube case, as the algorithm was trained on the user feedback, which in this case was biased and had personal prejudices, the algorithm started flagging the LGBTQ+ content as inappropriate. We discussed that there is not a clear solution for the problem and the only way forward would be for YouTube deflagging the videos manually.
In the Northpointe case study, relying on software to assess risk assessments of convicted criminals is not a good idea. We discussed some of the important points like we could make the algorithm open source so people can work and collaborate on it making it close to unbiased. One of the points was probably an ideal solution, which is simply not to factor in race and gender while training the algorithm, reducing the race bias. 
Some other points that we discussed were that we let people decide the outcome of the COMPAS, but the major flaw in this approach is that doing this can introduce more bias in the system as most of the people are biased. 
Some Solutions to reduce bias in COMPAS, Northpointe could employ fairness-aware algorithms that adjust predictions to reduce disparities in how different racial groups are treated. The company can also retrain its algorithm with newer and fairer data standards.